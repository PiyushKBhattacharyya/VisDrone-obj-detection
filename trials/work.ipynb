{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import torch\n",
    "import os\n",
    "import psutil\n",
    "from ultralytics import YOLO\n",
    "import time\n",
    "import numpy as np\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Dataset folder if it doesn't exist\n",
    "if not os.path.exists('Dataset'):\n",
    "    os.makedirs('Dataset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cpu, RAM: 15.35GB, CPU Cores: 8, Power Mode: MEDIUM\n"
     ]
    }
   ],
   "source": [
    "# Detect hardware capabilities\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "gpu_available = torch.cuda.is_available()\n",
    "total_ram = psutil.virtual_memory().total / (1024 ** 3)  # Convert bytes to GB\n",
    "cpu_cores = psutil.cpu_count(logical=False)  # Physical cores\n",
    "power_mode = \"HIGH\" if gpu_available and total_ram > 8 else \"MEDIUM\" if cpu_cores > 4 else \"LOW\"\n",
    "\n",
    "print(f\"Device: {device}, RAM: {total_ram:.2f}GB, CPU Cores: {cpu_cores}, Power Mode: {power_mode}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select appropriate YOLO model based on power mode\n",
    "if power_mode == \"HIGH\":\n",
    "    model_path = \"yolov8m.pt\"  # High model for high-end GPUs\n",
    "    frame_skip = 1  # Process every frame\n",
    "    conf_threshold = 0.25\n",
    "    fps_limit = 60\n",
    "elif power_mode == \"MEDIUM\":\n",
    "    model_path = \"yolov8s.pt\"  # Small model for mid-range devices\n",
    "    frame_skip = 2  # Process every 2nd frame\n",
    "    conf_threshold = 0.3\n",
    "    fps_limit = 45\n",
    "else:\n",
    "    model_path = \"yolov8n.pt\"  # Nano model for low-power devices\n",
    "    frame_skip = 3  # Process every 3rd frame\n",
    "    conf_threshold = 0.35\n",
    "    fps_limit = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tracking_threshold = 100  # Distance threshold for the same person\n",
    "histogram_threshold = 0.6  # Minimum histogram correlation to be considered the same person\n",
    "time_threshold = 5 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading https://github.com/ultralytics/assets/releases/download/v0.0.0/yolov8s.pt to yolov8s.pt...\n",
      "100%|██████████| 21.5M/21.5M [02:07<00:00, 177kB/s] \n"
     ]
    }
   ],
   "source": [
    "# Load YOLO model and send to device\n",
    "model = YOLO(model_path)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Open video stream (replace with drone RTSP URL if needed)\n",
    "video_source = 0  # Use drone RTSP stream if applicable\n",
    "cap = cv2.VideoCapture(video_source)\n",
    "\n",
    "# Set FPS limit dynamically for power efficiency\n",
    "cap.set(cv2.CAP_PROP_FPS, fps_limit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize tracking structures\n",
    "persons_tracker = []\n",
    "tracked_persons = {}\n",
    "frame_count = 0\n",
    "face_id_counter = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ultralytics YOLOv8.0.43  Python-3.10.11 torch-2.2.0+cpu CPU\n",
      "YOLOv8s summary (fused): 168 layers, 11156544 parameters, 0 gradients, 28.6 GFLOPs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved detected person image to: Dataset/person_20250219-215605_0.jpg\n",
      "Unique persons detected so far: 1\n",
      "Unique persons detected so far: 1\n",
      "Unique persons detected so far: 1\n",
      "Unique persons detected so far: 1\n",
      "Unique persons detected so far: 1\n",
      "Unique persons detected so far: 1\n",
      "Unique persons detected so far: 1\n",
      "Unique persons detected so far: 1\n",
      "Unique persons detected so far: 1\n",
      "Unique persons detected so far: 1\n",
      "Unique persons detected so far: 1\n",
      "Unique persons detected so far: 1\n",
      "Unique persons detected so far: 1\n",
      "Unique persons detected so far: 1\n",
      "Unique persons detected so far: 1\n",
      "Unique persons detected so far: 1\n",
      "Unique persons detected so far: 1\n",
      "Unique persons detected so far: 1\n",
      "Unique persons detected so far: 1\n",
      "Unique persons detected so far: 1\n",
      "Unique persons detected so far: 1\n",
      "Unique persons detected so far: 1\n",
      "Unique persons detected so far: 1\n",
      "Unique persons detected so far: 1\n",
      "Unique persons detected so far: 1\n",
      "Unique persons detected so far: 1\n",
      "Unique persons detected so far: 1\n",
      "Unique persons detected so far: 1\n",
      "Unique persons detected so far: 1\n",
      "Unique persons detected so far: 1\n",
      "Unique persons detected so far: 1\n",
      "Unique persons detected so far: 1\n",
      "Unique persons detected so far: 1\n",
      "Unique persons detected so far: 1\n",
      "Unique persons detected so far: 1\n",
      "Unique persons detected so far: 1\n",
      "Unique persons detected so far: 1\n",
      "Unique persons detected so far: 1\n",
      "Unique persons detected so far: 1\n",
      "Unique persons detected so far: 1\n",
      "Unique persons detected so far: 1\n",
      "Unique persons detected so far: 1\n",
      "Unique persons detected so far: 1\n",
      "Unique persons detected so far: 1\n",
      "Unique persons detected so far: 1\n",
      "Unique persons detected so far: 1\n",
      "Unique persons detected so far: 1\n",
      "Unique persons detected so far: 1\n",
      "Unique persons detected so far: 1\n",
      "Unique persons detected so far: 1\n",
      "Unique persons detected so far: 1\n",
      "Unique persons detected so far: 1\n",
      "Unique persons detected so far: 1\n",
      "Unique persons detected so far: 1\n",
      "Unique persons detected so far: 1\n",
      "Unique persons detected so far: 1\n",
      "Unique persons detected so far: 1\n",
      "Unique persons detected so far: 1\n",
      "Unique persons detected so far: 1\n",
      "Unique persons detected so far: 1\n",
      "Unique persons detected so far: 1\n",
      "Unique persons detected so far: 1\n",
      "Unique persons detected so far: 1\n",
      "Unique persons detected so far: 1\n",
      "Unique persons detected so far: 1\n",
      "Unique persons detected so far: 1\n",
      "Unique persons detected so far: 1\n",
      "Unique persons detected so far: 1\n",
      "Unique persons detected so far: 1\n",
      "Unique persons detected so far: 1\n",
      "Unique persons detected so far: 1\n",
      "Saved detected person image to: Dataset/person_20250219-215616_1.jpg\n",
      "Unique persons detected so far: 2\n",
      "Unique persons detected so far: 2\n",
      "Saved detected person image to: Dataset/person_20250219-215616_2.jpg\n",
      "Unique persons detected so far: 3\n",
      "Unique persons detected so far: 3\n",
      "Unique persons detected so far: 3\n",
      "Unique persons detected so far: 3\n",
      "Saved detected person image to: Dataset/person_20250219-215617_3.jpg\n",
      "Unique persons detected so far: 4\n",
      "Unique persons detected so far: 4\n",
      "Unique persons detected so far: 4\n",
      "Unique persons detected so far: 4\n",
      "Unique persons detected so far: 4\n",
      "Unique persons detected so far: 4\n",
      "Unique persons detected so far: 4\n",
      "Unique persons detected so far: 4\n",
      "Unique persons detected so far: 4\n",
      "Unique persons detected so far: 4\n",
      "Unique persons detected so far: 4\n",
      "Unique persons detected so far: 4\n",
      "Unique persons detected so far: 4\n",
      "Unique persons detected so far: 4\n",
      "Unique persons detected so far: 4\n",
      "Unique persons detected so far: 4\n",
      "Unique persons detected so far: 4\n",
      "Unique persons detected so far: 4\n",
      "Unique persons detected so far: 4\n",
      "Unique persons detected so far: 4\n",
      "Unique persons detected so far: 4\n",
      "Unique persons detected so far: 4\n",
      "Unique persons detected so far: 4\n",
      "Unique persons detected so far: 4\n",
      "Unique persons detected so far: 4\n",
      "Unique persons detected so far: 4\n",
      "Unique persons detected so far: 4\n",
      "Unique persons detected so far: 4\n",
      "Unique persons detected so far: 4\n",
      "Unique persons detected so far: 4\n",
      "Unique persons detected so far: 4\n",
      "Unique persons detected so far: 4\n",
      "Unique persons detected so far: 4\n",
      "Unique persons detected so far: 4\n",
      "Unique persons detected so far: 4\n",
      "Unique persons detected so far: 4\n",
      "Unique persons detected so far: 4\n",
      "Unique persons detected so far: 4\n",
      "Unique persons detected so far: 4\n",
      "Unique persons detected so far: 4\n",
      "Unique persons detected so far: 4\n",
      "Unique persons detected so far: 4\n",
      "Unique persons detected so far: 4\n",
      "Unique persons detected so far: 4\n",
      "Unique persons detected so far: 4\n",
      "Unique persons detected so far: 4\n",
      "Unique persons detected so far: 4\n",
      "Unique persons detected so far: 4\n",
      "Unique persons detected so far: 4\n",
      "Unique persons detected so far: 4\n",
      "Unique persons detected so far: 4\n",
      "Unique persons detected so far: 4\n",
      "Unique persons detected so far: 4\n",
      "Unique persons detected so far: 4\n",
      "Unique persons detected so far: 4\n",
      "Unique persons detected so far: 4\n",
      "Unique persons detected so far: 4\n",
      "Unique persons detected so far: 4\n",
      "Unique persons detected so far: 4\n",
      "Unique persons detected so far: 4\n",
      "Unique persons detected so far: 4\n",
      "Unique persons detected so far: 4\n",
      "Unique persons detected so far: 4\n",
      "Unique persons detected so far: 4\n",
      "Unique persons detected so far: 4\n",
      "Unique persons detected so far: 4\n",
      "Unique persons detected so far: 4\n",
      "Unique persons detected so far: 4\n",
      "Unique persons detected so far: 4\n"
     ]
    }
   ],
   "source": [
    "def euclidean_distance(p1, p2):\n",
    "    return np.linalg.norm(np.array(p1) - np.array(p2))\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        print(\"Failed to grab frame, exiting...\")\n",
    "        break\n",
    "\n",
    "    frame_count += 1\n",
    "    if frame_count % frame_skip != 0:\n",
    "        continue  # Skip frames dynamically to save power\n",
    "\n",
    "    # Flip the frame horizontally for mirroring effect\n",
    "    frame = cv2.flip(frame, 1)\n",
    "\n",
    "    # Resize frame to 640x480 for faster processing (comment if full resolution is needed)\n",
    "    frame = cv2.resize(frame, (640, 480))\n",
    "\n",
    "    # Run YOLO inference with streaming and lower confidence threshold\n",
    "    results = model(frame, verbose=False, stream=True, conf=conf_threshold)\n",
    "\n",
    "    # Process detections and draw bounding boxes\n",
    "    for result in results:\n",
    "        for box in result.boxes:\n",
    "            x1, y1, x2, y2 = map(int, box.xyxy[0])  # Bounding box coordinates\n",
    "            conf = box.conf[0].item()  # Confidence score\n",
    "            cls = int(box.cls[0].item())  # Class index\n",
    "\n",
    "            # Only process \"person\" class (class 0 for most YOLO models)\n",
    "            if model.names[cls] == \"person\" and conf >= conf_threshold:\n",
    "                # Compute the center of the bounding box\n",
    "                center_x = (x1 + x2) // 2\n",
    "                center_y = (y1 + y2) // 2\n",
    "                new_center = (center_x, center_y)\n",
    "\n",
    "                # Check if this person is already tracked\n",
    "                is_new_person = True\n",
    "                for existing_person in persons_tracker:\n",
    "                    # If the distance between centers is less than a threshold, it's the same person\n",
    "                    if euclidean_distance(new_center, existing_person) < 100:\n",
    "                        is_new_person = False\n",
    "                        break\n",
    "\n",
    "                if is_new_person:\n",
    "                    # If it's a new person, save the image and add it to the tracker\n",
    "                    cropped_image = frame[y1:y2, x1:x2]\n",
    "                    timestamp = time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "                    file_name = f\"Dataset/person_{timestamp}_{face_id_counter}.jpg\"\n",
    "                    cv2.imwrite(file_name, cropped_image)  # Save the image\n",
    "\n",
    "                    # Add the new person's center to the tracker\n",
    "                    persons_tracker.append(new_center)\n",
    "                    face_id_counter += 1\n",
    "                    print(f\"Saved detected person image to: {file_name}\")\n",
    "\n",
    "            # Skip drawing bounding boxes for non-person objects\n",
    "            if model.names[cls] != \"person\":\n",
    "                continue\n",
    "\n",
    "            # Draw bounding box for detected person\n",
    "            label = f\"{model.names[cls]} {conf:.2f}\"\n",
    "            cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "            cv2.putText(frame, label, (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
    "\n",
    "    # Display the frame\n",
    "    cv2.imshow(\"Adaptive Drone Detection (Mirrored)\", frame)\n",
    "\n",
    "    # Print the count of unique persons detected\n",
    "    print(f\"Unique persons detected so far: {len(persons_tracker)}\")\n",
    "\n",
    "    # Press 'q' to exit\n",
    "    if cv2.waitKey(1) & 0xFF == ord(\"q\"):\n",
    "        break\n",
    "\n",
    "# Release resources\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
